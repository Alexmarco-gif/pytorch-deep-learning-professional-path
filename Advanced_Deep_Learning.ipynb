{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0uvhsvEH8Z+crkq9HXwDA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexmarco-gif/pytorch-deep-learning-professional-path/blob/main/Advanced_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 1: Model Deployment & ONNX\n",
        "Saving and Exporting the Final Model <br>\n",
        "Task is to practice the serialization and export process using the ResNet-18 model trained in the previous Capstone project. You may not have the weights loaded right now, we will simulate the process by loading a fresh ResNet-18 instannce.\n",
        "\n",
        "Instruction:\n",
        "1. Instantiate the pre-trained ResNet-18 model and modify its final fc layer to output 10 classes.\n",
        "2. Set the model to evaluation mode (.eval()).\n",
        "3. Save the model's state dictionary to a file named resnet18_final.pth.\n",
        "4. Define a dummy input tensor matching the required input shape for CIFAR-10 images (Batch size 1, 3 channels, 32x32).\n",
        "5. Export the model to the ONNX format, naming the file resnet18_cifar10.onnx."
      ],
      "metadata": {
        "id": "Weg1hBkY9O9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSoQdvJE9AYa",
        "outputId": "250432bb-845e-4db0-871e-a0da497a9abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.19.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/tmp/ipython-input-1299438506.py:18: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# 1. Instantiate the pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify its final fc layer to output 10 classes (for CIFAR-10)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 10)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'resnet18_cifar10.pth')\n",
        "\n",
        "dummy_input = torch.rand(1, 3, 32, 32)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    'resnet18_cifar10.onnx',\n",
        "    export_params =True,\n",
        "    opset_version=17,      # ONNX standard version\n",
        "    do_constant_folding=True, # Optimize graph\n",
        "    input_names=['input'],\n",
        "    output_names=['output']\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 2: Recurrent Neural Networks (RNNs)\n",
        "Recurrent Neural Networks (RNNs) are designed specifically to handle sequences by maintaining an internal hidden state that acts as a memory of past inputs.\n",
        "1. The Simple RNN\n",
        "A simple RNN processes data one step (token/word/timestamp) at a time. At each step $t$, it takes the current input $x_t$ and the hidden state from the previous step $h_{t-1}$ to calculate the new hidden state $h_t$ and the output $y_t$:$$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$The same weights ($W_{hh}$, $W_{xh}$) are reused across all steps in the sequenceâ€”this is the principle of weight sharing in time\n",
        "2. Long Short-Term Memory (LSTM)\n",
        "LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) were invented to solve the vanishing gradient problem.\n",
        "* Forget Gate: Decides what information to throw away from the past state.\n",
        "* Input Gate: Decides what new information to add to the state.\n",
        "* Output Gate: Decides what to output based on the updated state.\n",
        "\n",
        "This gated structure allows gradients to flow smoothly, enabling the network to learn dependencies that span hundreds of time steps."
      ],
      "metadata": {
        "id": "L_S4OSbRFQxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ Exercise 1: Implementing an LSTM for Sequence Classification\n",
        "Your task is to define and run a forward pass through a basic LSTM network for a sequence classification task (e.g., sentiment analysis).\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Define a network called SentimentLSTM that inherits from `nn.Module`.\n",
        "\n",
        "2. The network must contain an `nn.LSTM` layer with the following settings:\n",
        "* input_size: 50 (representing the embedding size of each word/token).\n",
        "* Hidden_size: 100.\n",
        "* num_layers: 2.\n",
        "\n",
        "3. The network should have a final nn.Linear layer to map the final hidden state to 3 classes (e.g., positive, negative, neutral).\n",
        "\n",
        "4. Define the forward method:\n",
        "* The input tensor x will have the shape: (`Batch Size, Sequence Length, Input Size`).\n",
        "* The LSTM output is a tuple: (`output, (h_n, c_n)`). We only care about the final hidden state, $\\mathbf{h_n}$.\n",
        "* Use the final hidden state of the last layer (h_n[-1]) to feed the Linear layer.\n",
        "5. Run a forward pass with a dummy input: (`Batch=32, Sequence Length=15, Input Size=50`).\n",
        "\n",
        "6. Print the output logits shape."
      ],
      "metadata": {
        "id": "_td-P0oIGwgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import  torch.nn as nn\n",
        "\n",
        "class SentimentLSTM (nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SentimentLSTM, self).__init__()\n",
        "    # LSTM layer with input_size=50, hidden_size=100, num_layers=2\n",
        "    self.lstm = nn.LSTM(input_size=50, hidden_size=100, num_layers=2, batch_first=True)\n",
        "    # Linear layer to map the final hidden state to 3 classes\n",
        "    self.linear = nn.Linear(in_features=100, out_features=3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x shape: (Batch Size, Sequence Length, Input Size)\n",
        "    # lstm_output contains all hidden states, (h_n, c_n) contains final hidden and cell states\n",
        "    lstm_output, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "    # Use the final hidden state of the last layer (h_n[-1])\n",
        "    # h_n has shape (num_layers * num_directions, batch, hidden_size)\n",
        "    # h_n[-1] gets the hidden state from the last layer for all batches\n",
        "    logits = self.linear(h_n[-1])\n",
        "    return logits\n",
        "\n",
        "  # Instantiate the SentimentLSTM network\n",
        "  model = SentimentLSTM()\n",
        "\n",
        "  # Define a dummy input tensor: (Batch=32, Sequence Length=15, Input Size=50)\n",
        "  dummy_input = torch.randn(32, 15, 50)\n",
        "\n",
        "  # Perform a forward pass with the dummy input\n",
        "  output_logits = model(dummy_input)\n",
        "\n",
        "  # Print the output logits shape\n",
        "  print(\"Output logits shape:\", output_logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZEJD7WNFF6h",
        "outputId": "5932ad13-f677-45d0-e11c-1bb68617984b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output logits shape: torch.Size([32, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 3: Attention and Transformers\n",
        "Attention allows the model to selectively focus on the most relevant parts of the input sequence when producing an output, instead of relying on a single final hidden state.\n",
        "The process is:\n",
        "\n",
        "1. Calculate Attention Scores by taking the dot product of the Query vector with all Key vectors.\n",
        "\n",
        "2. Apply Softmax to these scores to create weights (probabilities) that sum to 1.\n",
        "\n",
        "3. The final output is a weighted sum of the Value vectors, where the weights determine how much the model \"pays attention\" to each part of the input.\n",
        "\n",
        "\n",
        "\n",
        "While building a full Transformer from scratch is complex, we can implement the core Scaled Dot-Product Attention mechanism itself.The scaled dot-product attention calculation is defined as:$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Softmax}(\\frac{\\mathbf{QK}^T}{\\sqrt{d_k}})\\mathbf{V}$$where $\\sqrt{d_k}$ is the scaling factor to prevent large values from pushing the Softmax into extreme regions."
      ],
      "metadata": {
        "id": "StlOxoj2MScU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ Exercise 2: Implementing Scaled Dot-Product Attention\n",
        "Your task is to implement the core Scaled Dot-Product Attention function in PyTorch.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Write a function scaled_dot_product_attention that accepts three tensors: Q, K, V.\n",
        "1. Assume the tensors are shaped (Batch, Head, Sequence Length, Feature Dimension).\n",
        "1. Calculate the raw attention scores using matrix multiplication of $\\mathbf{Q}$ and the transpose of $\\mathbf{K}$ (over the last two dimensions).\n",
        "1. Scale the scores by $\\frac{1}{\\sqrt{d_k}}$, where $d_k$ is the last dimension of the Key tensor ($\\mathbf{K}$).\n",
        "1. Apply Softmax to the scaled scores over the last dimension (the sequence length dimension).\n",
        "1. Calculate the final attention output by performing matrix multiplication of the attention weights and the Value tensor ($\\mathbf{V}$).\n",
        "1. Run a forward pass using the provided dummy tensors and print the final output shape."
      ],
      "metadata": {
        "id": "cuYyv8pj32fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "def scaled_dot_product_attention(Q,V,K):\n",
        "  k_trans = K.transpose(-2,-1)\n",
        "  attn_scores = torch.matmul(Q, k_trans)\n",
        "\n",
        "  d_k = K.shape[-1]\n",
        "  scaled_attn_scores = attn_scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "  attention_weights = torch.nn.functional.softmax(scaled_attn_scores, dim=-1)\n",
        "\n",
        "  output = torch.matmul(attention_weights, V)\n",
        "\n",
        "  return output\n",
        "\n",
        "# --- Setup Dummy Tensors ---\n",
        "# B=4 (Batch Size), H=8 (Attention Heads), S=10 (Sequence Length), Dk=64 (Feature Dimension)\n",
        "batch_size = 4\n",
        "num_heads = 8\n",
        "seq_len = 10\n",
        "d_k = 64 # Feature dimension\n",
        "\n",
        "# Q, K, V all have the same shape in self-attention: (B, H, S, Dk)\n",
        "Q = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
        "K = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
        "V = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
        "\n",
        "# Perform a forward pass\n",
        "output_tensor = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Output attention tensor shape:\", output_tensor.shape)"
      ],
      "metadata": {
        "id": "Wcok6NWbLK2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f237c424-b0a9-4a58-87b9-47054f609780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output attention tensor shape: torch.Size([4, 8, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the core computational component of the Transformer!"
      ],
      "metadata": {
        "id": "7YdyXEi57nwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 4: Advanced Model Visualization (Explainability)\n",
        "Model Explainability (or XAI) techniques help us visualize what the model is looking at. One of the most popular methods for CNNs is Grad-CAM (Gradient-weighted Class Activation Mapping).\n",
        "\n",
        "1. The Goal of Grad-CAM\n",
        "Grad-CAM produces a coarse, low-resolution heatmap that highlights the regions in the input image that were most important for the model's classification decision.\n",
        "\n",
        "2. The Grad-CAM Procedure\n",
        "Grad-CAM requires two key pieces of information from the network:\n",
        "\n",
        "1. Activations: The feature maps ($A$) from the final convolutional layer. These are the model's final, rich spatial features.Gradients:\n",
        "2. The gradients of the classification loss ($L$) with respect to those final feature maps ($\\frac{\\partial L}{\\partial A}$)."
      ],
      "metadata": {
        "id": "B4LKHxr17txO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ Exercise 4: Setting up PyTorch Hooks\n",
        "Before we build the full Grad-CAM function, you need to practice capturing the output of an intermediate layer using a forward hook. This is a fundamental technique in PyTorch inspection.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Instantiate a simple nn.Sequential model (like a mini-CNN).\n",
        "\n",
        "2. Define a dictionary activation_map to store the intermediate tensor.\n",
        "\n",
        "3. Define a function get_activation that takes the module, input, and output, and saves the output tensor to the activation_map dictionary.\n",
        "\n",
        "4. Register a Forward Hook on the first ReLU layer of the model, linking it to the get_activation function.\n",
        "\n",
        "5. Run a forward pass on a dummy input.\n",
        "\n",
        "6. Print the shape of the tensor saved in the activation_map."
      ],
      "metadata": {
        "id": "JQ_cTc5d8iCf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c59c6ec0",
        "outputId": "c8a57f7a-8d4e-4479-8c03-d764a9348106"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Define a simple neural network model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),  # This is the layer we will hook\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Linear(32 * 8 * 8, 10) # Assuming input 3x32x32 -> 32x8x8 features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten for linear layer\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n",
        "\n",
        "# 2. Initialize an empty dictionary to store activations\n",
        "activation_map = {}\n",
        "\n",
        "# 3. Define a hook function `get_activation(module, input, output)`\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation_map[name] = output.detach() # .detach() to prevent memory leaks\n",
        "    return hook\n",
        "\n",
        "# 4. Register the forward hook on the first ReLU layer\n",
        "# Access the first ReLU layer within the 'features' sequential module\n",
        "# model.features[1] corresponds to the first nn.ReLU()\n",
        "hook_handle = model.features[1].register_forward_hook(get_activation('first_relu'))\n",
        "\n",
        "# 5. Create a dummy input tensor\n",
        "# (Batch_size, Channels, Height, Width) -> (1, 3, 32, 32) for CIFAR-10 like input\n",
        "dummy_input = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "# 6. Perform a forward pass through the model\n",
        "_ = model(dummy_input)\n",
        "\n",
        "# 7. Print the shape of the stored activation\n",
        "print(\"Shape of the captured activation from 'first_relu' layer:\", activation_map['first_relu'].shape)\n",
        "\n",
        "# (Optional) Remove the hook after use to clean up resources\n",
        "hook_handle.remove()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the captured activation from 'first_relu' layer: torch.Size([1, 16, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ Exercise 5: Building and Running Grad-CAM\n",
        "Your task is to write the complete, runnable code to calculate and print the shape of the final Grad-CAM heatmap. We will simulate the forward/backward pass on your SimpleCNN model.\n",
        "\n",
        "Instructions:\n",
        "1. Define a dictionary gradient_map to store the gradients.\n",
        "2. Define a hook function get_gradient to store the gradient. The backward hook must be registered on the output tensor itself.\n",
        "3. Run a forward pass through your SimpleCNN and store the output $\\mathbf{A}$ of the target layer (model.features[4], the second Conv2D's output ReLU).\n",
        "4. Register the backward hook on this output tensor $\\mathbf{A}$ before the backward pass.\n",
        "5. Simulate Loss: Since we don't have true labels, select an arbitrary class (e.g., class 5) and backpropagate the gradient of that specific logit: model_output[0][5].backward(retain_graph=True).\n",
        "6. Calculate the Heatmap: Implement steps 2, 3, and 4 of the Grad-CAM algorithm using the captured $\\mathbf{A}$ (from the forward hook) and $\\mathbf{G}$ (from the backward hook).\n",
        "7. Print the final heatmap shape."
      ],
      "metadata": {
        "id": "ZvJ60WRYQQL-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abvSb2h4Qweo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08c55c79",
        "outputId": "3ae5ece2-2430-4f8f-e7f9-738f07ea2d56"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Re-instantiate the model for a clean run if needed, or use the existing one\n",
        "# (assuming model from Exercise 4 is still in scope)\n",
        "model = SimpleCNN()\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "# 1. Define a dictionary to store gradients\n",
        "gradient_map = {}\n",
        "\n",
        "# 2. Define a hook function to store the gradient (for module backward hook)\n",
        "def get_gradient(name):\n",
        "    # For module.register_backward_hook, the hook receives (module, grad_input, grad_output)\n",
        "    def hook(module, grad_input, grad_output):\n",
        "        # grad_output is a tuple of gradients for each output; we need the first one.\n",
        "        gradient_map[name] = grad_output[0].detach()\n",
        "    return hook\n",
        "\n",
        "# Define a dictionary to store activations\n",
        "activation_map_grad_cam = {}\n",
        "def get_activation_grad_cam(name):\n",
        "    def hook(model, input, output):\n",
        "        activation_map_grad_cam[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "# Register forward hook on the target layer (model.features[4], the second ReLU)\n",
        "forward_hook_handle = model.features[4].register_forward_hook(get_activation_grad_cam('target_layer_activation'))\n",
        "\n",
        "# Register backward hook on the target layer module (model.features[4])\n",
        "# This hook will capture gradients flowing back through this module's output.\n",
        "backward_hook_handle = model.features[4].register_backward_hook(get_gradient('target_layer_gradient'))\n",
        "\n",
        "# Create a dummy input tensor\n",
        "dummy_input = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "# 3. Run a forward pass through your SimpleCNN\n",
        "model_output = model(dummy_input)\n",
        "\n",
        "# Get the activation from the target layer (captured by forward hook)\n",
        "activations = activation_map_grad_cam['target_layer_activation']\n",
        "\n",
        "# 5. Simulate Loss: Select an arbitrary class (e.g., class 5) and backpropagate\n",
        "target_class = 5\n",
        "# The .backward() call will now trigger the module's backward hook.\n",
        "model_output[:, target_class].sum().backward(retain_graph=True)\n",
        "\n",
        "# Remove hooks after use to clean up resources\n",
        "forward_hook_handle.remove()\n",
        "backward_hook_handle.remove() # Corrected typo here\n",
        "\n",
        "# Retrieve the captured gradients (G) and activations (A)\n",
        "gradients = gradient_map['target_layer_gradient']\n",
        "activations = activation_map_grad_cam['target_layer_activation']\n",
        "\n",
        "# 6. Calculate the Heatmap (Grad-CAM algorithm)\n",
        "\n",
        "# Global average pooling of gradients to get neuron importance weights (alpha_k)\n",
        "# Gradients shape: (1, 32, 8, 8) -> (Batch, Channels, Height, Width)\n",
        "# Pool over spatial dimensions (Height, Width)\n",
        "weights = F.adaptive_avg_pool2d(gradients, 1)\n",
        "\n",
        "# Multiply weights with activations and sum along the channel dimension\n",
        "# Weights shape: (1, 32, 1, 1)\n",
        "# Activations shape: (1, 32, 8, 8)\n",
        "# Resulting heatmap (before ReLU) shape: (1, 32, 8, 8) -> (1, 8, 8) after sum\n",
        "heatmap = (weights * activations).sum(dim=1, keepdim=True)\n",
        "\n",
        "# Apply ReLU to the heatmap\n",
        "heatmap = F.relu(heatmap)\n",
        "\n",
        "# 7. Print the final heatmap shape\n",
        "print(\"Final Grad-CAM heatmap shape:\", heatmap.shape)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Grad-CAM heatmap shape: torch.Size([1, 1, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That brings us to the end of this session, there are more project end to end pipelines coming up soon"
      ],
      "metadata": {
        "id": "jTbWJf7XVymD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ePeK5q58SImB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}