{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPq+/6PXgY+AM3Fo5+Zu0mk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexmarco-gif/pytorch-deep-learning-professional-path/blob/main/Intermediate_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2: Intermediate\n",
        "## Convolutional Neural Networks (CNNs)\n",
        "Module 1: Convolutional Layers (nn.Conv2d)`"
      ],
      "metadata": {
        "id": "QAtF4BwGrttv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convulational Network\n",
        "Convolutional Layers (nn.Conv2d) offer a more efficient approach to image processing than linear layers by using a small kernel that slides over the image, sharing parameters across the entire input and preserving spatial relationships. This kernel performs element-wise multiplication and sums the results to create an output feature map; multiple kernels allow the detection of various features.\n",
        "### Understanding Shape Transformation\n",
        "The most important thing to track is the shape of your data. The input shape for nn.Conv2d is always (Batch, Channels, Height, Width) or (B, C, H, W).The output Height ($H_{out}$) and Width ($W_{out}$) are calculated with this formula:\n",
        "$$H_{out} = \\lfloor \\frac{H_{in} + 2P - K}{S} \\rfloor + 1$$ <br>\n",
        "* H in‚Äã = Input Height\n",
        "* P = Padding\n",
        "* K = Kernel Size\n",
        "* S = Stride <br>\n",
        "(The same formula applies for Width.)\n"
      ],
      "metadata": {
        "id": "pVpkFIkLkJ-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pAVS2iurRdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ca3529-7057-43d2-cf37-3dc8be1e0123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the input shape==> torch.Size([64, 1, 28, 28])\n",
            "the output shape ===> torch.Size([64, 10, 26, 26])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# 1. Define the input tensor (Batch, Channels, Height, Width)\n",
        "# 64 samples (a batch), 1 channel (grayscale), 28x28 image\n",
        "input_data = torch.rand(64, 1, 28,28)\n",
        "\n",
        "# 2. Define the Convolutional Layer\n",
        "# in_channels=1 (Input is grayscale)\n",
        "# out_channels=10 (We will learn 10 different features)\n",
        "# kernel_size=3 (A 3x3 filter)\n",
        "# stride=1, padding=0 (Defaults)\n",
        "conv_layer = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=0)\n",
        "\n",
        "# 3. Perform the Forward Pass\n",
        "out_features_shape = conv_layer(input_data)\n",
        "\n",
        "print(f\"the input shape==> {input_data.shape}\")\n",
        "print(f\"the output shape ===> {out_features_shape.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Exercise 1: Mastering Convolutional Shapes with Padding\n",
        "A common trick is to use padding to make the output image the same size as the input image.\n",
        "\n",
        "Your task is to figure out the correct padding value and verify it.\n",
        "\n",
        "Setup:\n",
        "* Input Image Shape: (1, 1, 32, 32)\n",
        "* Kernel Size: kernel_size=5\n",
        "* Stride: stride=1\n",
        "\n",
        "1. Prediction: What value for padding will make the output height and width 32 (the same as the input)? (Hint: Plug $H_{out} = 32$ into the formula and solve for $P$).\n",
        "2. Implementation: Define an input tensor with the correct shape.\n",
        "3. Verification: Define a nn.Conv2d layer (you can pick any out_channels) using your predicted padding value, pass the input through it, and print the output shape to confirm it is (1, C_out, 32, 32)."
      ],
      "metadata": {
        "id": "kXJRaaa8pKne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_tensor = torch.rand(1,1,32,32)\n",
        "\n",
        "conv_layer = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "out_features_shape = conv_layer(input_tensor)\n",
        "\n",
        "print(f'the shape for the output tensor==> {out_features_shape.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9z4owJ9JUZn",
        "outputId": "f6a9c94c-8ed0-4d40-e1c8-c181535fdeeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the shape for the output tensor==> torch.Size([1, 16, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 2: Pooling and Architectures\n",
        "A Pooling Layer is to further process the feature maps. Pooling layers serve two main purposes\n",
        "1. Dimensionality Reduction\n",
        "2. Increase Robustness <br>\n",
        "The most common type is Max Pooling (`nn.MaxPool2d`)"
      ],
      "metadata": {
        "id": "vp1w85WjNtVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.rand(64, 16, 10, 10)\n",
        "# This will reduce the H and W by half (4 -> 2)\n",
        "# And define the kernel_size and stride\n",
        "pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "output = pool_layer(input)\n",
        "\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy214yjYLEgo",
        "outputId": "d252b5bd-f001-4583-b9f6-0ac49e70ec29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([64, 16, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining Layers: The CNN Architecture\n",
        "A typical CNN architecture is built by stacking blocks of layers:$$\\text{Input} \\rightarrow \\mathbf{Conv} \\rightarrow \\text{ReLU} \\rightarrow \\mathbf{Pool} \\rightarrow \\text{[Repeat]} \\rightarrow \\mathbf{Flatten} \\rightarrow \\mathbf{Linear} \\rightarrow \\text{Output}$$The transition from the final convolutional block to the initial linear (fully connected) layers requires a crucial step: **Flattening**."
      ],
      "metadata": {
        "id": "KJ8GwuwLPhMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN, self).__init__()\n",
        "\n",
        "    # Convolutional Block 1\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
        "    relu1 = nn.Relu()\n",
        "    pool1 = nn.MaxPool2d(kernel_sixe=2, stride=2)\n",
        "\n",
        "    # Convolutional Block 2\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "    relu2 = nn.ReLu()\n",
        "    pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Fully Connected (Linear) Layers\n",
        "    # We need to know the size of the flattened tensor (will calculate in exercise)\n",
        "    # Assuming input 28x28, it will flatten to 16 * 4 * 4 = 256\n",
        "    self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "    self.fc2 = nn.Linear(120, 10) # 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Block 1\n",
        "      x= self.pool1(self.relu1(self.conv1(x)))\n",
        "      # Block 2\n",
        "      x= self.pool2(self.relu2(self.conv2(x)))\n",
        "      # Flatten\n",
        "      x = x.view(x.size(0), -1)\n",
        "      # Fully Connected Layers)\n",
        "      x = self.fc1(x)\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ItSbfrFsPGWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üìù Exercise 2: Building and Testing a CNN\n",
        "Your task is to define a slightly different CNN and verify the dimension of the feature map before the flattening step.<br> Instructions: <br>\n",
        "1. Define a CNN class called `MiniVGG` using the following sequence:\n",
        "  * Input: 1 channel, $32 \\times 32$ image.\n",
        "  * Layer 1: nn.Conv2d (In=1, Out=8, Kernel=3, Padding=1), followed by $\\mathbf{nn.ReLU()}$.\n",
        "  * Layer 2: nn.MaxPool2d (Kernel=2, Stride=2).\n",
        "  * Layer 3: nn.Conv2d (In=8, Out=16, Kernel=3, Padding=1), followed by $\\mathbf{nn.ReLU()}$.\n",
        "  * Layer 4: nn.MaxPool2d (Kernel=2, Stride=2).\n",
        "2. Perform a forward pass on a dummy batch of $16$ images (torch.randn(16, 1, 32, 32))\n",
        "3. Calculate the exact size of the feature map (C, H, W) after the final pooling layer (Layer 4).\n",
        "4. Print the shape of the tensor at that point."
      ],
      "metadata": {
        "id": "Wn4DAe2UTSKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MiniVGG (nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MiniVGG, self).__init__()\n",
        "\n",
        "    # Convolutional Block 1\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Convolutional Block 2\n",
        "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "  # Define a forward pass\n",
        "  def forward(self, x):\n",
        "    # Block 1\n",
        "    x = self.pool1(self.relu1(self.conv1(x)))\n",
        "\n",
        "    # Block 2\n",
        "    x = self.pool2(self.relu2(self.conv2(x)))\n",
        "\n",
        "    return x\n",
        "\n",
        "# Create a dummy batch of 16 images (torch.randn(16, 1, 32, 32))\n",
        "dummy_input = torch.randn(16, 1, 32, 32)\n",
        "\n",
        "# Create an instance for the MiniVGG\n",
        "model = MiniVGG()\n",
        "\n",
        "# Perform a forward pass\n",
        "output_tensor = model(dummy_input)\n",
        "\n",
        "# Print the shape of the tensor after the final pooling layer\n",
        "print(f\"Shape of the tensor after the final pooling layer: {output_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzszg2TPTEaz",
        "outputId": "3a9787ed-9f0e-4c99-b0ef-6faf03e0e435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the tensor after the final pooling layer: torch.Size([16, 16, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 3: Advanced DataLoaders & Augmentation\n",
        "Data augmentation involves applying random, yet realistic, transformations to the training images every time they are loaded. This makes the model see a slightly new version of the image each time, effectively increasing the size and diversity of the training set."
      ],
      "metadata": {
        "id": "hI5bFQ1jb-uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define transformations for the training set (with augmentation)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),       # Pad by 4 pixels, then randomly crop back to 32x32\n",
        "    transforms.RandomHorizontalFlip(),          # 50% chance of a horizontal flip\n",
        "    transforms.ToTensor(),                      # Convert to tensor\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # Normalize\n",
        "])\n",
        "\n",
        "# Define transformations for the test/validation set (NO augmentation, only standardization)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Convert to tensor\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # Normalize\n",
        "])"
      ],
      "metadata": {
        "id": "paUUKgC0alhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Exercise 3: Implementing CIFAR-10 Data Setup\n",
        "CIFAR-10 is a common benchmark dataset with $32 \\times 32$ color (RGB, 3-channel) images, classified into 10 classes (like 'airplane', 'dog', 'cat'). <br>\n",
        "Your task is to implement the full data pipeline for CIFAR-10 using the standard augmentation and normalization defined above.\n",
        "\n",
        "Instructions: <br>\n",
        "1. Define the train_transform and test_transform pipelines using transforms.Compose and the standard CIFAR-10 normalization values.\n",
        "2. Load the CIFAR-10 dataset for both training and testing, applying the correct transform to each.\n",
        "3. Create a DataLoader for the training set (use batch_size=128, shuffle=True).Iterate through the first batch of the train_loader.\n",
        "4. Print the shape of the image batch and the minimum and maximum pixel values of the images in that batch.<br>\n",
        "(The min/max values should be close to $-2$ and $+2$ due to the normalization.)"
      ],
      "metadata": {
        "id": "LUB2cWsxFzUG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "956b8381",
        "outputId": "21886891-1218-4c16-9699-900e8a106057"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define transformations for the training set (with augmentation)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Define transformations for the test/validation set (NO augmentation, only standardization)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Load the CIFAR-10 training dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# Load the CIFAR-10 testing dataset\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "print(\"CIFAR-10 training dataset loaded with augmented transformations.\")\n",
        "print(\"CIFAR-10 testing dataset loaded with standardized transformations.\")\n",
        "\n",
        "\n",
        "\n",
        "import torch.utils.data as data\n",
        "\n",
        "# Create a DataLoader for the training set\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Iterate through the first batch of the train_loader\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# Print the shape of the image batch\n",
        "print(f\"Shape of the image batch: {images.shape}\")\n",
        "\n",
        "# Print the minimum and maximum pixel values of the images in that batch\n",
        "print(f\"Minimum pixel value: {images.min()}\")\n",
        "print(f\"Maximum pixel value: {images.max()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 training dataset loaded with augmented transformations.\n",
            "CIFAR-10 testing dataset loaded with standardized transformations.\n",
            "Shape of the image batch: torch.Size([128, 3, 32, 32])\n",
            "Minimum pixel value: -2.429065704345703\n",
            "Maximum pixel value: 2.7537312507629395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 4: Training and Optimization\n",
        "In this module, we introduce three essential components that are crucial for training deep, complex networks like CNNs to achieve high accuracy without crashing or taking too long: Batch Normalization, Dropout, and Learning Rate Scheduling.\n",
        "\n"
      ],
      "metadata": {
        "id": "GE84f-cGLJ26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Exercise 4: Integrating Optimization Techniques\n",
        "Your task is to modify the MiniVGG architecture (from Exercise 2) and update the training setup to include these new optimization methods.\n",
        "\n",
        "Instructions: <br>\n",
        "1. Update the MiniVGG class (__init__ and forward):\n",
        "  * Add nn.BatchNorm2d after every nn.Conv2d layer.\n",
        "  * Add nn.Dropout (with p=0.5) before the first Linear layer (after the flattening step).\n",
        "2. Add Linear Layers: Complete the network by adding the final two Linear layers after the dropout and flattening step.\n",
        "  * Input to first Linear layer is $16 \\times 8 \\times 8 = 1024$.\n",
        "  * Hidden Linear layer of size $\\mathbf{120}$.\n",
        "  * Output Linear layer of size $\\mathbf{10}$ (for 10 classes).\n",
        "3. Setup the Scheduler: Instantiate a StepLR scheduler that reduces the learning rate by a factor of 0.1 every 3 epochs.\n",
        "\n",
        "<br>\n",
        "You only need to show the full updated MiniVGG class and the scheduler instantiation. Assume the training loop is updated to call scheduler.step() at the end of each epoch."
      ],
      "metadata": {
        "id": "a7MciMfVL4HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MiniVGG (nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MiniVGG, self).__init__()\n",
        "\n",
        "    # Convolutional Block 1\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(8) # Batch Normalization after Conv1\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Convolutional Block 2\n",
        "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(16) # Batch Normalization after Conv2\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    # Fully Connected (Linear) Layers\n",
        "    # Input to first Linear layer is 16 * 8 * 8 = 1024\n",
        "    self.fc1 = nn.Linear(16 * 8 * 8, 120)\n",
        "    self.fc2 = nn.Linear(120, 10) # 10 classes\n",
        "\n",
        "  # Define a forward pass\n",
        "  def forward(self, x):\n",
        "    # Block 1\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    # Flatten\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    # Dropout and Fully Connected Layers\n",
        "    x = self.dropout(x) # Apply dropout before the first linear layer\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "# Create a dummy batch of 16 images (torch.randn(16, 1, 32, 32))\n",
        "dummy_input = torch.randn(16, 1, 32, 32)\n",
        "\n",
        "# Create an instance for the MiniVGG\n",
        "model = MiniVGG()\n",
        "\n",
        "# Perform a forward pass\n",
        "output_tensor = model(dummy_input)\n",
        "\n",
        "# Define an optimizer (e.g., SGD) before instantiating the scheduler\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "\n",
        "# Print the shape of the tensor after the final pooling layer\n",
        "# Note: This print statement is for the output of the full network, not just after pooling.\n",
        "print(f\"Shape of the tensor after the final linear layer: {output_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epp6nKthL1P9",
        "outputId": "f054dff2-c711-46e2-c889-4c702d620174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the tensor after the final linear layer: torch.Size([16, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 5: Introduction to Transfer Learning\n",
        "Transfer learning is the concept of using a model pre-trained on one task to improve performance on a second, related task. Instead of starting from scratch, which requires vast amounts of data and computational power, you leverage the knowledge gained from the first task to build a new model more efficiently and accurately.\n",
        "### Two Main Techniques\n",
        "* Feature Extractor (Freezing)\n",
        "* Fine-Tuning (Unfreezing)\n"
      ],
      "metadata": {
        "id": "QVl4k2hPSOpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "# 1. Load the pre-trained ResNet-18 model\n",
        "# 'weights=models.ResNet18_Weights.IMAGENET1K_V1' loads the weights trained on ImageNet\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# 2. Freeze all the layers in the model\n",
        "for param in resnet18.parameters():\n",
        "  param.requires_grad= False\n",
        "\n",
        "# 3. Replace the final fully connected layer (the Classifier Head)\n",
        "# ResNet's classifier is called 'fc'. We need to get the input features of the original fc layer.\n",
        "num_features = resnet18.fc.in_features\n",
        "\n",
        "# Replace the existing 'fc' layer with a new one tailored for 10 classes\n",
        "# The parameter of this new layer are unfreezen and they will be trained\n",
        "resnet18.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "# Verify that only the new 'fc' layer parameters require gradients\n",
        "total_params = sum(p.numel() for p in resnet18.parameters())\n",
        "trainable_param = sum(p.numel() for p in resnet18.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "print(f\"Number of trainable parameters: {trainable_param:,}\")"
      ],
      "metadata": {
        "id": "rFAgvAgRMte0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d6850b-b486-4512-a6db-213640f126e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 11,181,642\n",
            "Number of trainable parameters: 5,130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Modifying a Different Model\n",
        "Your task is to perform the \"Feature Extractor\" transfer learning technique on a different pre-trained model: VGG-16. Assume your target classification problem has 3 classes (e.g., a custom dataset).\n",
        "\n",
        "Instructions: <br>\n",
        "1. Load the pre-trained VGG-16 model.\n",
        "2. Freeze all parameters in the model's feature extraction layers.\n",
        "3. Replace the final classifier layer to output 3 classes. VGG-16's classifier is an `nn.Sequential` block, and the final layer is `vgg16.classifier[6]`.\n",
        "* You need to find the input features for this last layer (`vgg16.classifier[6].in_features`).\n",
        "4. Print the Total parameters and the Trainable parameters to verify only the new layer is trainable."
      ],
      "metadata": {
        "id": "JtMJh8t_YoQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# 1. Load the pre-trained VGG-16 model\n",
        "model_vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# 2. Freeze all parameters in the model's feature extraction layers\n",
        "for param in model_vgg16.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# 3. Replace the final classifier layer to output 3 classes.\n",
        "# VGG-16's classifier is an nn.Sequential block, and the final layer is vgg16.classifier[6].\n",
        "# Get the input features for this last layer\n",
        "num_features_vgg = model_vgg16.classifier[6].in_features\n",
        "\n",
        "# Replace the existing final layer with a new one tailored for 3 classes\n",
        "model_vgg16.classifier[6] = nn.Linear(num_features_vgg, 3)\n",
        "\n",
        "# 4. Print the Total parameters and the Trainable parameters to verify only the new layer is trainable.\n",
        "total_params_vgg = sum(p.numel() for p in model_vgg16.parameters())\n",
        "trainable_params_vgg = sum(p.numel() for p in model_vgg16.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total number of parameters in VGG-16: {total_params_vgg:,}\")\n",
        "print(f\"Number of trainable parameters in VGG-16: {trainable_params_vgg:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkSq4keJVy_K",
        "outputId": "389b9349-e821-4bd1-a15f-52300ae97187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in VGG-16: 134,272,835\n",
            "Number of trainable parameters in VGG-16: 12,291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2 Capstone Project: Transfer Learning with ResNet-18\n",
        "You will now combine all knowledge from Phase 2 (CNN architecture, data preparation, optimization, and transfer learning) to classify the CIFAR-10 dataset using a pre-trained ResNet-18 model.\n",
        "<br>\n",
        "Project Goal: Achieve a test set accuracy of greater than 88% after a few epochs using transfer learning.\n",
        "\n",
        "üìã Project Specifications\n",
        "1. Data Setup (Module 3)\n",
        "* Use the CIFAR-10 dataset.\n",
        "* Implement the full augmentation (RandomCrop, RandomHorizontalFlip) and Normalization pipeline for the training set.\n",
        "* Implement only Normalization for the test set.\n",
        "* DataLoaders: batch_size=128 (train), batch_size=100 (test).\n",
        "\n",
        "2. Model Setup (Module 5: Feature Extractor)\n",
        "* Load pre-trained ResNet-18 (weights=models.ResNet18_Weights.IMAGENET1K_V1).\n",
        "* Freeze ALL the parameters in the model's base layers (resnet18.parameters()).\n",
        "* Replace the final fully connected layer (resnet18.fc) to output 10 classes (CIFAR-10 has 10 classes).\n",
        "\n",
        "3. Training & Optimization (Module 4)\n",
        "* Device: Use CUDA/CPU checking and move model/data to the correct device.\n",
        "* Optimizer: `torch.optim.SGD` with a high learning rate, e.g., $\\mathbf{0.01}$ (because we are only training a small, new layer).\n",
        "* Loss: `nn.CrossEntropyLoss()`.\n",
        "* Scheduler: Use `StepLR` to drop the learning rate by $\\mathbf{0.1}$ every $\\mathbf{7}$ epochs.Epochs: Run for $\\mathbf{10}$ epochs.\n",
        "\n",
        "4. Training and Testing Loops\n",
        "* Reuse your train_loop and test_loop functions from the Phase 1 Capstone, but remember to call `scheduler.step()` at the end of the train_loop after `optimizer.step()`.\n",
        "\n",
        "üíª Your Task\n",
        "Write the complete, runnable Python script that accomplishes the entire Transfer Learning training process for CIFAR-10 and reports the final accuracy."
      ],
      "metadata": {
        "id": "ljuXVwnRf291"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "\n",
        "# Data Setup\n",
        "training_set_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip (),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "test_set_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "training_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=training_set_transform)\n",
        "\n",
        "testing_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_set_transform)\n",
        "\n",
        "train_loader = data.DataLoader(training_set, batch_size=128, shuffle=True)\n",
        "test_loader = data.DataLoader(testing_set, batch_size=100, shuffle=True)\n",
        "\n",
        "# Model Setup\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze all layers initially\n",
        "for param in resnet18.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer4 and layer3 for fine-tuning\n",
        "for param in resnet18.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in resnet18.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "num_features = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "\n",
        "# Training and Optimization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet18.to(device)\n",
        "\n",
        "# Use a smaller learning rate for fine-tuning\n",
        "optimizer = torch.optim.SGD(resnet18.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Training and Testing Loops\n",
        "def train_loop(model, train_loader, optimizer, criterion, device):\n",
        "  model.train()\n",
        "  for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def test_loop(model, test_loader, criterion, device):\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      test_loss += criterion(outputs, labels).item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  accuracy = 100.0 * correct / len(test_loader.dataset)\n",
        "  return test_loss, accuracy\n",
        "\n",
        "# Increased epochs for fine-tuning\n",
        "for epoch in range(30):\n",
        "  train_loop(resnet18, train_loader, optimizer, criterion, device)\n",
        "  test_loss, accuracy = test_loop(resnet18, test_loader, criterion, device)\n",
        "  scheduler.step()\n",
        "  print(f\"Epoch {epoch+1}/{30}, Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "pMntFNw4egX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "14be733f",
        "outputId": "bc998ba7-4fa3-4c12-dd48-af213539fcba"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get a batch of training images and labels\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# Select the first image and label from the batch\n",
        "img = images[5]\n",
        "label = labels[5]\n",
        "\n",
        "# Denormalize the image for display\n",
        "# The original mean and std were: ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
        "std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
        "img = img * std + mean # Undo normalization\n",
        "\n",
        "# Convert to numpy array and transpose dimensions for matplotlib (C, H, W) -> (H, W, C)\n",
        "np_img = img.numpy().transpose((1, 2, 0))\n",
        "\n",
        "# Clip values to be between 0 and 1, as some values might be slightly outside due to floating point operations\n",
        "np_img = np.clip(np_img, 0, 1)\n",
        "\n",
        "# Get the class name for the label\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(np_img)\n",
        "plt.title(f\"Label: {class_names[label]}\")\n",
        "plt.axis('off') # Hide axes ticks\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHsdJREFUeJzt3XmQnAXV7/HTe/d0z74lDpkkM9mYGBahgrK8YQt5vUbe5C0E/eNiCi9FIVyVMihWAYGySq7lApRaSKlsBaVFAKlwLxK5kghI3oQAL4RcYtZJMpNJJrOv3dPLc//wesqQITknotzR7+cvaX5zeJ5+uvs3TyZzDAVBEAgAACIS/qgPAADw/w9KAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgF/c+3t7RIKheT73//+hzZz48aNEgqFZOPGjR/azMncddddEgqFpsxc4K9FKWBSjzzyiIRCIdm6detHfSgA/o4oBeAEbr/9dhkfH/+oDwP4u6EUgBOIRqOSTCZPmCmVSpLNZv9ORwT8bVEKOGUTExNy5513yjnnnCOVlZWSTqfloosukg0bNnzg19x7770yc+ZMSaVSsmTJEnn33XePy+zYsUOuuuoqqampkWQyKeeee66sW7fupMczNjYmO3bskJ6enpNmX3nlFfnc5z4nzc3NkkgkZMaMGXLLLbccd1cw2Z/9h0Ihufnmm+WJJ56QhQsXSiKRkBdeeOGYn51YzvP9Hn74Ybn00kuloaFBEomEtLW1yQMPPHBcbtasWbJ8+XJ59dVXZfHixZJMJqWlpUUee+yx47IDAwPyta99TWbMmCGJRELmzJkj3/3ud6VUKp30ePDPKfpRHwCmrqGhIfn5z38uX/jCF+T666+X4eFh+cUvfiHLli2TLVu2yFlnnXVM/rHHHpPh4WG56aabJJvNyv333y+XXnqpbNu2TRobG0VEZPv27XLBBRdIU1OT3HbbbZJOp+XJJ5+UFStWyNNPPy0rV678wOPZsmWLXHLJJbJmzRq56667Tnjsa9eulbGxMbnxxhultrZWtmzZIj/60Y+ko6ND1q5de9Jzf+mll+TJJ5+Um2++Werq6mTWrFmu85zMAw88IAsXLpQrr7xSotGoPPfcc/LlL39ZSqWS3HTTTcdkd+/eLVdddZV86Utfki9+8Yvy0EMPyapVq+Scc86RhQsXisifSnLJkiXS2dkpN9xwgzQ3N8trr70m3/rWt6Srq0vuu+++k54n/gkFwCQefvjhQESC119//QMzhUIhyOVyxzzW398fNDY2Btddd50+tm/fvkBEglQqFXR0dOjjmzdvDkQkuOWWW/Sxyy67LFi0aFGQzWb1sVKpFJx//vnB3Llz9bENGzYEIhJs2LDhuMfWrFlz0vMbGxs77rF77rknCIVCwf79+/WxNWvWBO9/m4hIEA6Hg+3btx/zuOc8J5s72TEtW7YsaGlpOeaxmTNnBiISvPzyy/pYd3d3kEgkgq9//ev62Le//e0gnU4HO3fuPObrb7vttiASiQQHDhw47r8H8MdHOGWRSETi8biI/OnP1fv6+qRQKMi5554rb7755nH5FStWSFNTk/7z4sWL5bzzzpPnn39eRET6+vrkpZdekquvvlqGh4elp6dHenp6pLe3V5YtWya7du2Szs7ODzyeiy++WIIgOOldgohIKpXS/z06Oio9PT1y/vnnSxAE8tZbb53065csWSJtbW2T/ruTnaflmAYHB6Wnp0eWLFkie/fulcHBwWOybW1tctFFF+k/19fXy/z582Xv3r362Nq1a+Wiiy6S6upqfS57enrk8ssvl2KxKC+//PJJzxP/fPjjI/xVHn30UfnBD34gO3bskHw+r4/Pnj37uOzcuXOPe2zevHny5JNPisif/kgkCAK544475I477pj0v9fd3X3MB+6pOnDggNx5552ybt066e/vP+bfvf8DeDKTnd+fnew8P8gf/vAHWbNmjWzatEnGxsaOO6bKykr95+bm5uO+vrq6+phz2bVrl7zzzjtSX18/6X+vu7v7hMeDf06UAk7Z448/LqtWrZIVK1bIrbfeKg0NDRKJROSee+6RPXv2uOf9+Yefq1evlmXLlk2amTNnzl91zCIixWJRli5dKn19ffLNb35TFixYIOl0Wjo7O2XVqlWmH8L+5Xf1H4Y9e/bIZZddJgsWLJAf/vCHMmPGDInH4/L888/Lvffee9wxRSKRSecEf/H/rlsqlWTp0qXyjW98Y9LsvHnzPrwTwD8MSgGn7KmnnpKWlhZ55plnjvkbOmvWrJk0v2vXruMe27lzp/6QtqWlRUREYrGYXH755R/+Af8/27Ztk507d8qjjz4q1157rT7+4osvfijzT3aek3nuueckl8vJunXrjrkLONHf5DqZ1tZWGRkZ+Zs+l/jHw88UcMr+/N3qX353unnzZtm0adOk+WefffaYnwls2bJFNm/eLJ/+9KdFRKShoUEuvvhiefDBB6Wrq+u4rz969OgJj8f6V1InO+4gCOT+++8/4ddZnew8rcc0ODgoDz/88Ckfx9VXXy2bNm2S9evXH/fvBgYGpFAonPJs/OPiTgEn9NBDD8kLL7xw3ONf/epXZfny5fLMM8/IypUr5TOf+Yzs27dPfvrTn0pbW5uMjIwc9zVz5syRCy+8UG688UbJ5XJy3333SW1t7TF/vPGTn/xELrzwQlm0aJFcf/310tLSIkeOHJFNmzZJR0eHvP322x94rNa/krpgwQJpbW2V1atXS2dnp1RUVMjTTz993M8WTpXlPN/viiuukHg8Lp/97GflhhtukJGREfnZz34mDQ0Nkxakxa233irr1q2T5cuX619XHR0dlW3btslTTz0l7e3tUldXd6qniX9QlAJOaLJfnhIRWbVqlaxatUoOHz4sDz74oKxfv17a2trk8ccfl7Vr1066qO7aa6+VcDgs9913n3R3d8vixYvlxz/+sUyfPl0zbW1tsnXrVrn77rvlkUcekd7eXmloaJCzzz5b7rzzzg/lnGKxmDz33HPyla98Re655x5JJpOycuVKufnmm+XMM8/8q+dbzvP95s+fL0899ZTcfvvtsnr1apk2bZrceOONUl9fL9ddd90pHUdZWZn8/ve/l+985zuydu1aeeyxx6SiokLmzZsnd9999zE/uAb+LBT85f0qgFPW3t4us2fPlu9973uyevXqj/pwgFPCzxQAAIpSAAAoSgEAoPiZAgBAcacAAFCUAgBAmX9Pgf+TcQCY2iw/LeBOAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoKIf9QGciuuuWWbODgx0u2aXVaTN2VKo5JqdSkbM2ca6StfsYm7MlS8ViuZsXVWFa3ZrXcqcLY9MuGaHi/Z8bCLvmp0p+vKxwJ7vz7lGSzYemLPhqO97u1DInh0r+l7jQSxhzhbGsr7Zed+xSDhmjmad1350wn7s0VDcNbuYs78302n755UVdwoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFBTcvfRlz//eXN2y5ZXXbN37dljznZ27XPNHgiNm7PFId++oekZ+84ZEZGaMvsembLxo67Z8YmkOZspt++nEREpif05HPfugwqXufL5iRFzNpx3LBwSkUzB/tasKPftvxkbtT8v4bx9B5OISDRhf90O9/W7ZoeCgitfjNn3E2XSvvdPvGR/Dgt5+84zEZGgaP9ePZnzHbcFdwoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAA1JRcc3H6GReas82tba7ZPb195uxrr/7ONfu3r6w3Z3f2+X6lfyDru5SzykfN2Ugw6Jo9OF5vzk6fVu2aPe5Yu1DIFl2zm9O+53BmXZM5Wxb2rfMYH7Gv88jGfcfdO2yfHU5mXLNHHSsaohU1rtnRkG9dxMRE3h6O+NZF5ELD5uyRIfvzLSIyOmo/7rKylGu2BXcKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQU3L3UXbCvv9GomWu2XXT0ubslSuuds1uOes8c/ZXW9tds3uGRlz5we7N5my+e4drdjRv392SzB5xzR7JD9mzOd/OmUK1/dqLiFQk7W+fkORcs+Nj9mMvC5e7Zs+ssu8c6h/27b3K5uzXvjzp29sTypdc+YqwfX9YKuHbq1QTOD6Dwo4dTCLSV8yaszHnfi8L7hQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAqCm55iIpY+bsRNGeFREpFCbM2Xhg/zV6EZFFLa3m7CtdvvUcQ2n7r8aLiPz7JZ8wZ8d2ve6avfv/bDVndx34o2t231i/OZsNfCsAJsIVrnxFr/3657O+12FG7LObk5Wu2ZGofaXDeMy3iqIYsc+O19a5ZhdGR135ge795uz4mO/9E0TtH53TWz7mmp0/ZH+N5/P2zysr7hQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKCm5O4jKdr3woR8628kJCFzNhqLuWZHZdyc7dr7nmv2ht/8hyv/3//HLebsFcu/4Jq9+7xPmrMv/e/fumZve+sVc/bQoV2u2cGIb//N3v3DjuG+779qGhrM2exQ3jU7nbDng5Bvdm7Cvp8oVkq7ZtdP9+14SqZOM2eHBntds4t5+0dnyLmDKxexv1aqG3x7lSy4UwAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgJqSu48KgX3nUDGw7zISEclJxDHb16kJx16l8taFrtkTC3w7al4fsO9hmuV4TkREmh07aq5Z+XnX7HMWnW3Ovrl5g2t2uGB/TkRE2g/bdyv19Nt3AomIDBy0X89UzHfcteX261lfl3HNjkerzNmjXb7npDASuPISt0eTldNdowd67M/hwKjvY7b8tLnmbHNzo2u2BXcKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAANSUXHMRBM5fd3eIRuyrKLyHkS+lzNlI4xmu2ZFP+fKHMxPm7JgMu2bnBvrN2Z6Q/TkRERmpaTZnr1jx31yza8uLrnxXd6c5+8bbO12zt7+71Zwd7u9wzT40al+LcXigyzW7PGFf/5BO2tfViIjEo2OufMGRrflYq2v2xz+xzJyty8xyzY6kEubsmQvqXbMtuFMAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICakruP7NuJREqlkmt2VLLmbKTk2a4iUozad72UAvsOGRGR4XyZK7/tSN6cfa22yjV7T4f9OWwf3uuafWFzuTn78SbnXpjCoCs+Z0aLOTt9+nzX7LPPPdec3b1vh2t2b98Rc3bnW1tcsw+17zFnQyHfe7MU+PKZ6mnm7Px/WeqaPZ6xX89dh4Zcsz959mnmbDSwv4+tuFMAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoKbkmotw+G/XZUExMGdDMd/Tl4/b89m4c4VGre/X3bc6Vgy07yu6ZldnZpmzCxqqXbOTZb3m7OGi7zlMhFKufG7nTnP26FHfCo3WM84wZ5f9l2tcswtiv/bdF+5yzd7x5lZz9oXfbXTNTmUqXPmV11xrzmaDjGv2UN7+OVGW8H1OhEYOm7NbX3zJNfuCaz910gx3CgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUFNz91EkYs869yQNl+w7ULrHQq7ZR8fsO2d6yhKu2fOmu+ISPTpgzgbDo67Z0+rqzdlZJd/1OfRenznb1VDnmn1ay2mu/OxpcXP25Y2/dM3euOVdc/Zz11ztmt00e5Y5m0jZr6WIyLyFi83ZhpZzXLMrG6a58o3NLebs6KhvT1aq3H7td2x/2zX7hV//1JyNjtlfJyIiFxjWQXGnAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBNyTUXnl9ILwaBa/aBfNqcfbFz0DW749BOc7aYOOqafUHUd57vbnzNnK2trHLNrhlpNGdDEd/3JROOlRudPUOu2clq31qM05vmmrNLrvyvrtntb9ivz1tv2V9XIiLv7e42Z/cf3Oeanc9nzdnGab61Io2DvrUy2cC+KmZmU5NrdkXcPrtrr+85fOPt183ZqrjvM8iCOwUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAKgpufsoCEXM2aL49qXkSiVztrKx2jU7lu81Z6f1H3HNrs8kXflUxL4zZXQ455od2ddvztbMnuWaPft0+76cvX2+/VGVPV2u/K7D9vkNFTWu2edf8q/mbM/RHtfsra+/bc5ufse3tycUt3+kJPb1uWZXp/e78pWpN83ZGS0VrtmXLP0Xc3YiO+Ca/dl/XWrO/vHtt1yzLbhTAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAmpK7j4r5gjkbcvberPCIOTtypMM1OzsxbM42lMVds48c8e1KakjZdyX1Ze37oEREGptqzdlI1H4tRUS2vLHZnA1nMq7ZdRX1rnxfl33H08GhHa7ZbeedYc6WV1W5Zn/iAvvenmlz57lmH+iw70rq7vTtmuo75NuVNN6XNWf7Y/asiMjMqP09MT57jmt265h9p9r5H/+Ua7YFdwoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAA1JRccxEN2btsLJtzzR46uNecrRgecM1OheyrJQ4dPOiavb/dvl5ARGRkYNycLUYirtn5IfvLKldKu2a/sfU1c3baHN+KhsWLfSsDDnXZV4uEyxOu2aWEPT/m20Iioah9hUpD0wzX7IamaeZsdsi+UkZEZLTXvlZERKTnkP369Ff7XuNHY83m7KvFGtfswaHd5mw62umabcGdAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAA1JTcfZQdtu9M2b97j2t2VAJzNhWvcs0+eKDbnB0Y8e1sqsqkXPn6tH3nUE921DV7sMu+uyXTYN+VIyKy8tNLzdn3untcszP1Fa58rNb+HCZSGdfsfJA3Z0u5kGt2OGRflhSJ2/ckiYiEwvaPlFDUtw+q4DhuEZHT5lWZs5lMg2v2nj1Zc3b9Zt+OJ6mwPy8LEoO+2QbcKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQU3LNxcH2/eZsedq3XiAcsvfkgf0drtmRoGjO1tdVumY3Zmpd+bRjfcF4xH7cIiKjw/b1EqM9Q67ZpZx9vUAyX3DNrgzHXPlPzD/dnB0YHXfNLsskzdli0b6aRUTEsclFhrNjrtF5x/eZYd/LSrp7h135g10D5mym2bcmJlyyv38y725yza5os78Oa8/yrWax4E4BAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAABqSu4+mt7UZM6WAt9emMGBQXO2utq3n6i6ptycjZfZd6uIiFRW+Ha3JCIhczYf8u0QKuTs+6YOxY+4Zu/sGDBn58+e65pdFbfvGxIRCXKj5mw2n3fNHjhk3x8173TnedZMM2cP9drPUUSkq7vPnB0e6nfNrsqkXfncuP16TgzZ3/ciIuVF+3l+sdW39+q8efbzrKlKuGZbcKcAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQE3JNRdlFfY1CsMjI67ZqXL7KopowrdaIhYt2rOxkm92KubKh8P2NRfFMd+qg1LIfn0ys3yrCxbOtOeLEd/16eo96sofONhhzuZyOdfsaMj+/dq6tc+4Zv/b1f9uztZX1rlmb3n5HXN2+39udc2OlJwra3rt7/3xUNY1e/7M6eZsJjfkmt1U+TFzNhn2rcOx4E4BAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAABqSu4+kpB9b0887tsNkoja96vko759KfajFonGIr7Zzh0oQ9mCOTtesu+DEhFJN9r35dTV+nbrjBfs+6MiYd/LOxlJuPKFkv05/OUvf+ma/W+fWW7OxhKtrtm/euIJc3Yim3fNXv8/nzdnw0XfPqhExPc97ED/sDlb1eR7HTZm7K/DQnHCNbuiwv5+i8U9nyo23CkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUFNyzUU4bO+yUqnkm+1YXRCP2FdiiIgUHcddcP72et73m/RSSlSbs/Fq3wqAYiZjzk4kfKslomn79UnHY67Z6ZJvtcjiT55jzuYmxlyzf/fCi+ZsVXWVa/bTv/qVORtkfasoKhP2j5TairRrdsz5LWxZJGnOtrae5pqdz42Ys+U19veaiEjIscYnEv3wP8K5UwAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgJqSu48KBfv+m7wjKyIS5O27XiJR34KiQtj+dIeTKdfsWLLSlZeoYz9RrNw1OpKy7zMKO5/DeMyed46WfMG3QOrgvgPm7HvvbXfN/s1v/5c5OzIw7JodcuwDq3RcSxGRxkr762p6nW8nUH1tlStf3WDf2TWztdU1e/0rr5iztXW+3WEh8bxwnS9yA+4UAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgpuTuoyAIzNlI2Nd7xaj9Kcm6JotEyqrM2bJpH3PNLkV9O2qK9vU3ko77Zifj9ue8zLHLSEQkWrLPPtxp300kIrL+t79x5X/9zK/N2T/u/KNrdpAvmrPlqTLX7HgQMWcrUmnX7KrKGnO2oa7BNXt6Y60rP2v2DHO2sfE012wp2l+HVdW+447GPtqPZe4UAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAKipueZC7GsuCiXHPgfxrYtI19a5Zieq7L/uXkykXLNLjtUSIiIpsa+XiEecswP7cz7QecQ1+81Nr5uzzz77tGv2a//5H6786OiYORuO2ldLiIgsXLTInK11rJYQEdnzzjZztq6qyjW7urranM2UV7pmp9PlrnwsHDNnx0fHXbNDYftHZ12d73OikC+Ys3HHcVhxpwAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAADUldx8VSvbdILFMxjU7WVtvzobLfLMl7Nl/49vZFAn7+j1Zsu+PyvcOuGZve9u+W+cPG3/vmv36ps3m7IHDB12zExHf2yFRbt/Fk8/nXbPrahvN2UzGt/soFt1pzs5o8M2e1mDf71VTY9+TJCJS5dzDFI3adx+NFrOu2cm0/f1WXpZ0zc6P218r0ZBvp5YFdwoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAA1JRccyGBY0XDxIRv9ti4OZpOpV2jU3H7r92XSr41F2ODw678gfYOc3b7ljdds7e/ac93dBxwzZ7I2q9PeVmZa3Ys5FtFUSja81nn9exs32fOlpX7rn2mLGXOzmia5ppdW2Nfi5FI+tY/pJIJV15C9s+JQsm7hqTScRi+a18s2Nf45HP2rBV3CgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUFNy91Ehb99TMjqSc80e7x0yZ6OHu12zaxx7YXJZ33G379rtynfu3W/OHm737Sca6usxZ0NB0TU7Erbvs6lw7PgREamI+vIl504bj94h++twxz7f9Vly7pnm7PTp9a7ZmfIKczaZ8O0yikYivnzM/j1vNOo7lk+efbY5m4jZd56JiESijo/lkGu0CXcKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAANSUXHMRCdu7LBn2nWKoUDBnR/v6XLP37LevIxgdGXHNHurrd+XHeuzHXsyOuWYH+aw5Gy7ZV5aIiJTF7NczlvBd+3jSmY/FzdlUKumaLY5VBwNZ33PYNneGOVtbX+WaHXOsiygvL3fNDntXOoTsK1FKjqyISHWy0pyNBM7PoJD9REecnxMW3CkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAECZl3IEgW83CABg6uFOAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoP4vZfLzAHC0Ca8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I4c-shw2iMhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}