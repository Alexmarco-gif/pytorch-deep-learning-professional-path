{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexmarco-gif/pytorch-deep-learning-professional-path/blob/main/Deep_Learning_Foundation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fpn-KbeaD61"
      },
      "source": [
        "## Creating Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCTT5lZ3REgh"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHIz1IaJWI5l",
        "outputId": "db5fc3cd-1ac2-4284-8db3-e1e227fb3d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor from the list of tensor([[1, 2],\n",
            "        [2, 6]])\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "data = [[1,2], [2,6]]\n",
        "x_data = torch.tensor(data)\n",
        "print(f\"tensor from the list of {x_data}\")\n",
        "print(x_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsne7RosXyM-"
      },
      "source": [
        "From Numpy Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI95a11JWmKt"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taOyuXdwX7c-",
        "outputId": "697d8449-5c91-44ba-c1f1-badfbc391269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numpy from list of tensor([[1, 2],\n",
            "        [2, 6]])\n"
          ]
        }
      ],
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print(f\"numpy from list of {x_np}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RyvIHBMYsbZ"
      },
      "source": [
        "Initialize with random value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eu8LFQ2Ymc3",
        "outputId": "6dbc5f1f-e56b-48d7-d0ad-252972bd566c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2x2 Matrices of Ones tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "2x2 Matrices of random value tensor([[0.3668, 0.8455],\n",
            "        [0.9860, 0.1436]])\n"
          ]
        }
      ],
      "source": [
        "x_ones = torch.ones(2, 2) # A 2 x 2 Matrix of ones\n",
        "x_rand = torch.rand (2, 2)# A 2x2 Matrix of random value uniform distributions\n",
        "\n",
        "print(f\"2x2 Matrices of Ones {x_ones}\")\n",
        "print(f\"2x2 Matrices of random value {x_rand}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x_p72s0Z_1_"
      },
      "source": [
        "## Tensor Attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNinbfaDZk3i",
        "outputId": "c1375bf4-efa0-43ab-9b57-4fec2a2b8655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "torch = torch.rand (3, 4)\n",
        "\n",
        "print(f\"Shape of tensor: {torch.shape}\")\n",
        "print(f\"Datatype of tensor: {torch.dtype}\")\n",
        "print(f\"Device tensor is stored on: {torch.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp7mz91BawF4",
        "outputId": "b328a18a-5528-4848-aff8-51c3c57f3737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA GPU is not avaliable\n",
            "Tensor on device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  print(\"NVIDIA GPU is avaliable\")\n",
        "else:\n",
        "  device = 'cpu'\n",
        "  print(\"NVIDIA GPU is not avaliable\")\n",
        "\n",
        "tensor_on_device = torch.ones (5,5)\n",
        "tensor_on_device = tensor_on_device.to(device) # Corrected variable name\n",
        "\n",
        "print(f\"Tensor on device: {tensor_on_device.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lEGrh2wciqy"
      },
      "source": [
        "## üìù Exercise 1: Tensor Manipulation and Device Check\n",
        "Your task is to create two specific tensors and report their attributes.\n",
        "1. Create a $4 \\times 3$ tensor named A filled with zeros, using the data type torch.float32.\n",
        "2. Create a $5 \\times 5$ tensor named B filled with random integers between 1 and 10.\n",
        "3. Move Tensor A to the GPU if one is available.\n",
        "4. Print the shape, data type, and device for both Tensor A and Tensor B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Hpotuhb0Uc",
        "outputId": "d45d9889-d6eb-493f-b161-b627d6525bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the shape of tensor 1 is torch.Size([4, 3])\n",
            "the shape of tensor 2 is torch.Size([5, 5])\n",
            "the data type of tensor 1 is torch.float32\n",
            "the data type of tensor 2 is torch.int64\n",
            "the device of tensor 1 is cpu\n",
            "the device of tensor 2 is cpu\n"
          ]
        }
      ],
      "source": [
        "tensor_1 = torch.zeros(4, 3, dtype=torch.float32 device=device)\n",
        "tensor_2 = torch.randint(1, 11, (5, 5))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "tensor_1 = tensor_1.to(device)\n",
        "\n",
        "\n",
        "print(f\"the shape of tensor 1 is {tensor_1.shape}\")\n",
        "print(f\"the shape of tensor 2 is {tensor_2.shape}\")\n",
        "print(f\"the data type of tensor 1 is {tensor_1.dtype}\")\n",
        "print(f\"the data type of tensor 2 is {tensor_2.dtype}\")\n",
        "print(f\"the device of tensor 1 is {tensor_1.device}\")\n",
        "print(f\"the device of tensor 2 is {tensor_2.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8R2jALAfJTC"
      },
      "source": [
        "# Module 2: Autograd and Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iihPXRB1xRK-"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lnAKkRFd_-S",
        "outputId": "5f00a40a-81c8-4d5c-8fb4-7045a1ca4bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "requires gradTrue\n",
            "z's operation function (grad_fn): <AddBackward0 object at 0x7a095d3bfa00>\n"
          ]
        }
      ],
      "source": [
        "# Tracking Operations\n",
        "\n",
        "x = torch.tensor(4.6, requires_grad=True)\n",
        "print(f'requires grad{x.requires_grad}')\n",
        "\n",
        "\n",
        "y = 2 * x\n",
        "z = y + 5\n",
        "\n",
        "\n",
        "print(f\"z's operation function (grad_fn): {z.grad_fn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEug0ItXyRe5"
      },
      "source": [
        "The Backpropagation Step <br>\n",
        "Let's calculate the derivative of $z$ with respect to $x$.$$z = 2x + 5$$The analytical derivative (gradient) is:$$\\frac{\\partial z}{\\partial x} = 2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqUDXa-KxMX7",
        "outputId": "252701bd-ad10-4ae4-dbf6-59c894f34867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The gradient (dz/dx) is: 2.0\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(4.6, requires_grad=True)\n",
        "y = x * 2\n",
        "z = y + 5\n",
        "\n",
        "# 1. Perform backpropagation\n",
        "# Since z is a scalar (single number), we call .backward() with no arguments\n",
        "z.backward()\n",
        "\n",
        "# 2. Access the gradient: dz/dx\n",
        "print(f\"The gradient (dz/dx) is: {x.grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DoOcvJIzU4R",
        "outputId": "adbe708a-0ec1-4f64-cea3-438e0d4058cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z's operation function (grad_fn): None\n"
          ]
        }
      ],
      "source": [
        "# Preventing Gradient Tracking\n",
        "x = torch.tensor(7.5, requires_grad=True)\n",
        "\n",
        "with torch.no_grad(): # Saves memory and computational times\n",
        "  y = x * 2\n",
        "  z = y + 5\n",
        "\n",
        "print(f\"z's operation function (grad_fn): {z.grad_fn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phBtTA3U0evK"
      },
      "source": [
        "## üìù Exercise 2: Implementing Autograd\n",
        "Your task is to calculate the gradient of the function $L$ with respect to $w$ and $b$, given the fixed input $x$. <br>\n",
        "The function is: $$L = (w \\cdot x + b)^2$$ Given values: <br>\n",
        "$x = 2.0$ <br>$w = 3.0$ <br>$b = 1.0$ <br>\n",
        "1. Initialize $x$, $w$, and $b$ as scalar tensors. Ensure only $w$ and $b$ have requires_grad=True (as $x$ is input data, not a parameter we optimize).\n",
        "2. Calculate the value of $L$.\n",
        "3. Perform the backpropagation step.\n",
        "4. Print the gradients $\\frac{\\partial L}{\\partial w}$ (stored in $w.grad$) and $\\frac{\\partial L}{\\partial b}$ (stored in $b.grad$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9Fm2W380Peg",
        "outputId": "4b508275-b5e8-4339-930d-a15dbfe1b4ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dL/dw: 28.0\n",
            "dL/db: 14.0\n"
          ]
        }
      ],
      "source": [
        "w = torch.tensor(3.0, requires_grad=True)\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "x = torch.tensor(2.0)\n",
        "\n",
        "y = w * x + b\n",
        "L = y ** 2\n",
        "\n",
        "L.backward()\n",
        "\n",
        "print(f\"dL/dw: {w.grad}\")\n",
        "print(f\"dL/db: {b.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub6EZ41r4Z_z"
      },
      "source": [
        "# Module 3: Building Basic Networks `(torch.nn)`\n",
        "`torch.Tensor` and `autograd` handle the math. <br>\n",
        "`torch.nn` module provides the high-level building blocks for constructing neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqmY6Dfe5ev5"
      },
      "source": [
        "The Linear Layer `(nn.Linear)`<br> This is the most fundamental layer, implementing a simple linear transformation:$$y = xA^T + b$$where $x$ is the input, $A$ is the weight matrix, and $b$ is the bias vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "IA1N2_xu3ZyY",
        "outputId": "251aa773-6c7a-4caa-d96a-7e34a803f6c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Structure:\n",
            "SimpleFNN(\n",
            "  (layer1): Linear(in_features=4, out_features=8, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (out): Linear(in_features=8, out_features=1, bias=True)\n",
            ")\n",
            "Model Parameters:\n",
            "Name ==> layer1.weight: Shape ==> torch.Size([8, 4])\n",
            "Name ==> layer1.bias: Shape ==> torch.Size([8])\n",
            "Name ==> out.weight: Shape ==> torch.Size([1, 8])\n",
            "Name ==> out.bias: Shape ==> torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SimpleFNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size ):\n",
        "    super(SimpleFNN, self).__init__()\n",
        "\n",
        "\n",
        "    # Layer 1: Input to Hidden\n",
        "    self.layer1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
        "\n",
        "    # Activation Function: Introduce non-linearity\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    # Layer 2: Hidden to Output\n",
        "    self.out= nn.Linear (in_features = hidden_size, out_features=output_size)\n",
        "\n",
        "\n",
        "\n",
        "  # Forward Pass: Define how data flows through the layers\n",
        "  def forward(self, x):\n",
        "    # x -> Linear Layer 1\n",
        "    x = self.layer1(x)\n",
        "    # -> ReLU Activation\n",
        "    x = self.relu(x)\n",
        "    # -> Linear Layer 2 (Final output)\n",
        "    logit = self.out(x)\n",
        "    return logit\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 4\n",
        "hidden_dim = 8\n",
        "output_dim = 1\n",
        "model = SimpleFNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(\"Model Structure:\")\n",
        "print(model)\n",
        "\n",
        "# View wthe parameters\n",
        "print(\"Model Parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "  print(f\"Name ==> {name}: Shape ==> {param.shape}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rBuP7kH7g-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61bc2ddd-1ff1-4a5b-db34-4fd55facf309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Parameters\n",
            "MNIST_Classifier(\n",
            "  (layer1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (layer2): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "torch.Size([32, 10])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MNIST_Classifier (nn.Module):\n",
        "  def __init__(self, input_size, hidden_layer_size_1, hidden_layer_size_2, output_size):\n",
        "    super(MNIST_Classifier, self).__init__()\n",
        "\n",
        "    # define the first layer\n",
        "    self.layer1 = nn.Linear(input_size, hidden_layer_size_1)\n",
        "\n",
        "    # Define ReLU activation function\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    # define second layer\n",
        "    self.layer2 = nn.Linear(hidden_layer_size_1, hidden_layer_size_2) # Corrected input size for layer2\n",
        "\n",
        "    # define the output layer\n",
        "    self.layer_out = nn.Linear(hidden_layer_size_2, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # pass the input to the first layer\n",
        "    x = self.layer1(x) # Corrected layer call\n",
        "\n",
        "    # apply the ReLU activation function\n",
        "    x = self.relu(x)\n",
        "\n",
        "    # pass the result to the second layer\n",
        "    x = self.layer2(x)\n",
        "\n",
        "    # apply the activation function\n",
        "    x = self.relu(x)\n",
        "\n",
        "    # pass the result to the output layer\n",
        "    logit = self.layer_out(x)\n",
        "    return logit\n",
        "\n",
        "# initialize the model\n",
        "Input = 784\n",
        "hidden_layer_1 = 256\n",
        "hidden_layer_2 = 64\n",
        "output_size = 10\n",
        "\n",
        "# instantiate the MNIST_Classifier model with the define sizes\n",
        "model = MNIST_Classifier(Input, hidden_layer_1, hidden_layer_2, output_size)\n",
        "\n",
        "# print the model structure\n",
        "\n",
        "print(\"Model Parameters\")\n",
        "print(model)\n",
        "\n",
        "\n",
        "# make prediction\n",
        "input_try = torch.rand(32, Input)\n",
        "output_pred = model(input_try)\n",
        "\n",
        "print(output_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 4: Training Essentials\n",
        "Now that we can define a model, we need the components that allow it to learn‚Äîthe loss function and the optimizer. This leads us to the Core Training Loop. <br> <br>\n",
        "1. Loss Functions (nn.Loss) The Loss Function (or Criterion) quantifies how far the model's prediction (logits) is from the true target value (label). The goal of training is to minimize this loss.\n",
        "* `MSELoss()`\n",
        "* `nn.CrossEntropyLoss()`\n",
        "* `nn.BCELoss()`\n",
        "2.  Optimizers (torch.optim) The Optimizer dictates how the model's parameters (weights and biases) are updated based on the gradients calculated by Autograd.\n",
        "* SGD\tStochastic Gradient Descent. \t`optim.SGD(params, lr)`\n",
        "* Adam\tAdaptive Moment Estimation. \t`optim.Ad`\n",
        "3. The Core Training Loop (Four Steps) <br>\n",
        "Training any PyTorch model involves repeating these four steps for every batch of data (one epoch):  <br>\n",
        "1. Zero Gradients: Clear any previous gradients stored in the parameters using `optimizer.zero_grad()`.\n",
        "2. Forward Pass: Calculate the model's output `(logits)` for the current batch: `logits = model(input)`.\n",
        "3. Calculate Loss: Compute the error: `loss = criterion(logits, labels)`.\n",
        "4. Backward Pass & Step:\n",
        "  * `loss.backward()`: Calculate the gradients ($\\frac{\\partial Loss}{\\partial \\text{Weights}}$).\n",
        "  * `optimizer.step()`: Update the parameters using the calculated gradients.\n"
      ],
      "metadata": {
        "id": "BX0MVUZHjaFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Exercise 4: Implementing the Training Core\n",
        "Your task is to set up the necessary components and simulate one step of the training loop for the MNIST_Classifier you just created.\n",
        "\n",
        "Instructions: <br>\n",
        "1. Reuse your MNIST_Classifier class and instantiate the model.\n",
        "2. Define the Loss Function (appropriate for 10-class classification) and the Optimizer (using Adam with a learning rate of 0.001).\n",
        "3. Create a dummy input batch (input_data, size $32 \\times 784$) and a dummy label tensor (labels, size $32$). The labels should be integers between 0 and 9.\n",
        "4. Implement the Four Steps of the training loop:\n",
        "    * Zero gradients.\n",
        "    * Forward pass to get logits.\n",
        "    * Calculate loss.\n",
        "    * Backward pass and optimizer step.\n",
        "5. Print the initial loss"
      ],
      "metadata": {
        "id": "BeS9cfxupHTo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FMQPGe-9Qhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf6d805b-b309-47e1-9004-9d26a0aaa735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.285759687423706\n"
          ]
        }
      ],
      "source": [
        "model = MNIST_Classifier(Input, hidden_layer_1, hidden_layer_2, output_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "dummy_input_batch = torch.rand(32, 784)\n",
        "dummy_label_tensor = torch.randint(0, 10, (32,))\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "logits = model(dummy_input_batch)\n",
        "loss = criterion(logits, dummy_label_tensor)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(f'loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 5: Data Handling (Dataset and DataLoader)"
      ],
      "metadata": {
        "id": "75_h-TKO4F9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# input data\n",
        "# 100 samples, 4 features each\n",
        "X_features = np.random.rand(100, 4).astype(np.float32)\n",
        "y_label = np.random.randint(0, 2, 100)\n",
        "\n",
        "# Define Custom Dataset\n",
        "class CustomTensorDataset():\n",
        "  def __init__(self, features, labels):\n",
        "    # Convert NumPy arrays to PyTorch Tensors\n",
        "    self.X = torch.tensor(features)\n",
        "    self.Y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    # The total size of the dataset\n",
        "    return len(self.Y)\n",
        "\n",
        "  def __getitem__ (self, idx):\n",
        "    # Returns one (feature, label) pair at the given index\n",
        "    return self.X[idx], self.Y[idx]\n",
        "\n",
        "# Instantiate Dataset and DataLoader\n",
        "dataset = CustomTensorDataset(X_features, y_label)\n",
        "\n",
        "# DataLoader setup:\n",
        "# batch_size=16: 16 samples per batch\n",
        "# shuffle=True: data is re-shuffled after every epoch\n",
        "# num_workers=2: load data using 2 parallel processes (recommended for real data)\n",
        "train_loader = DataLoader(\n",
        " dataset,\n",
        " batch_size = 16,\n",
        " shuffle = True,\n",
        " num_workers = 0 # Set to 0 for simple Colab/Jupyter for stability, use >0 in production\n",
        ")\n",
        "\n",
        "# Demonstrate Iteration\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "print(f\"Number of batches (100 samples / 16 batch size): {len(train_loader)}\")\n",
        "\n",
        "\n",
        "# Iterate over the DataLoader (simulating one epoch of training)\n",
        "for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "    print(f\"\\n--- Batch {batch_idx+1} ---\")\n",
        "    print(f\"Features batch shape: {features.shape}\")\n",
        "    print(f\"Labels batch shape: {labels.shape}\")\n",
        "    if batch_idx == 1:\n",
        "        break # Stop after 2 batches for brevity\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cCNvrqD4Xi8",
        "outputId": "6bf00500-f07c-4f95-e38c-f6aff17ee76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 100\n",
            "Number of batches (100 samples / 16 batch size): 7\n",
            "\n",
            "--- Batch 1 ---\n",
            "Features batch shape: torch.Size([16, 4])\n",
            "Labels batch shape: torch.Size([16])\n",
            "\n",
            "--- Batch 2 ---\n",
            "Features batch shape: torch.Size([16, 4])\n",
            "Labels batch shape: torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Exercise 5: Preparing Data for MNIST\n",
        "Your task is to set up the DataLoader for the MNIST training set. <br>\n",
        "Instructions: <br>\n",
        "1. Import the necessary classes (`datasets` and `transforms` from `torchvision`).\n",
        "2. Define a transformation to convert the PIL image into a PyTorch Tensor.\n",
        "3. Load the Training Data from `torchvision.datasets.MNIST`.\n",
        "4. Define the DataLoader with the following settings: <br>\n",
        "  * `batch_size`: 64\n",
        "  * `shuffle`: True\n",
        "  * `num_workers` = 0\n",
        "5. Iterate through the first batch of the `DataLoader` and print the shape of the image batch and the shape of the label batch."
      ],
      "metadata": {
        "id": "R6CQymTSiIQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define your transform for coverting PIL image to tensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load the Minist training data\n",
        "minst_traindata = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# define the DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(minst_traindata, batch_size=64, shuffle=True, num_workers=0)\n",
        "\n",
        "# iterate through the first batch\n",
        "for img, labels in train_loader:\n",
        "  print(f\"image batch shape: {img.shape}\")\n",
        "  print(f\"label batch shape: {labels.shape}\")\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBd1Xz3tjd1G",
        "outputId": "d1755867-fc83-4bc9-aff7-26e9eba3baf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 56.4MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 1.63MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 14.4MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 9.40MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image batch shape: torch.Size([64, 1, 28, 28])\n",
            "label batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1 Capstone Project: Fashion-MNIST Classifier"
      ],
      "metadata": {
        "id": "GNcrZZEPltNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Project Goal: Achieve at least 80% accuracy on the test set.\n",
        "#### üìã Project Specifications\n",
        "1. Data Setup (Using `torchvision`)\n",
        "  * Load the Fashion-MNIST dataset for both training (`train=True`) and testing (`train=False`).\n",
        "  * Apply the `transforms.ToTensor()` to both datasets.\n",
        "  * Create a `DataLoader` for both the training set (use `batch_size=64`, `shuffle=True`) and the test set (use `batch_size=1000`, `shuffle=False`).\n",
        "\n",
        "2. Model Definition (Reuse Your Network)\n",
        "  * Reuse your `MNIST_Classifier` structure (`Input: 784, Hidden 1: 256, Hidden 2: 64, Output: 10`).\n",
        "3. Training Setup:\n",
        "  * Device: Check for GPU availability and move the model and data to the correct device (CPU or CUDA).\n",
        "  * Loss: `nn.CrossEntropyLoss()`.\n",
        "  * Optimizer: `torch.optim.Adam` with `lr=0.001`.\n",
        "  * Epochs: Run for 5 epochs.\n",
        "4. The Training and Testing Loops <br>\n",
        "You need two functions/loops:\n",
        "    1. `train_loop(dataloader, model, loss_fn, optimizer):`\n",
        "        * Iterates over the training dataloader.\n",
        "        * Performs the 4-step training process (zero grad, forward, backward, step).\n",
        "        * Prints the average loss at the end of the epoch.\n",
        "    2. `test_loop(dataloader, model, loss_fn)`:\n",
        "        * Sets the model to evaluation mode (model.eval()) and disables gradient tracking (with torch.no_grad():).\n",
        "        * Calculates the average loss and the overall accuracy (number of correct predictions / total samples).\n",
        "        * CRITICAL STEP: Use torch.argmax(logits, dim=1) to convert the output logits into the predicted class indices.\n",
        "        * Sets the model back to training mode (model.train()) afterward.\n",
        "    \n",
        "\n",
        "üíª **Task**\n",
        "Write the complete, runnable Python script that accomplishes the entire training and testing process and reports the final accuracy.\n",
        "\n",
        "Note: You must modify your existing MNIST_Classifier class to accept the device parameter and move layers to it in the` __init__ `method, or simply move the entire model to the device after instantiation `(model.to(device))`. The latter is simpler."
      ],
      "metadata": {
        "id": "6myhUipjl7Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "ccOFRVjpo6f-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Device\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "# Define Transformer\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load MINIST Dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define Model\n",
        "class FashionMNIST_Classifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_layer_size_1, hidden_layer_size_2, output_size):\n",
        "    super(FashionMNIST_Classifier, self).__init__()\n",
        "    self.layer1 = nn.Linear(input_size, hidden_layer_size_1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.layer2 = nn.Linear(hidden_layer_size_1, hidden_layer_size_2)\n",
        "    self.layer_out = nn.Linear(hidden_layer_size_2, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.relu(x)\n",
        "    logit = self.layer_out(x)\n",
        "    return logit\n",
        "\n",
        "model = FashionMNIST_Classifier(784, 256, 64, 10).to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Define Training Loop Function\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train() # set model training\n",
        "  running_loss = 0.0\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Move data to the correct device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    pred = model(X.view(X.size(0), -1))\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimize steps\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if batch % 100 == 99:\n",
        "      current = (batch + 1) * len(X)\n",
        "      print(f\"Loss: {running_loss / 100:.4f}  [{current}/{size}]\")\n",
        "      running_loss = 0.0\n",
        "\n",
        "\n",
        "# Define Testing Loop Function\n",
        "def test_loop (dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval() # Set model to evaluation mode\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "\n",
        "      pred = model(X.view(X.size(0), -1)) # Flatten the image\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "  model.train()\n",
        "\n",
        "\n",
        "# Run Training and Testing\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, model, criterion, optimizer)\n",
        "  test_loop(test_loader, model, criterion)\n",
        "  print(\"Done !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhQiSfFlZ-2U",
        "outputId": "146e5c1a-41c5-450a-f13f-761feb16a355"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Loss: 1.0373  [6400/60000]\n",
            "Loss: 0.5938  [12800/60000]\n",
            "Loss: 0.5184  [19200/60000]\n",
            "Loss: 0.4974  [25600/60000]\n",
            "Loss: 0.4799  [32000/60000]\n",
            "Loss: 0.4644  [38400/60000]\n",
            "Loss: 0.4422  [44800/60000]\n",
            "Loss: 0.4202  [51200/60000]\n",
            "Loss: 0.4197  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.424610 \n",
            "\n",
            "Done !\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Loss: 0.3982  [6400/60000]\n",
            "Loss: 0.3976  [12800/60000]\n",
            "Loss: 0.3939  [19200/60000]\n",
            "Loss: 0.3820  [25600/60000]\n",
            "Loss: 0.3596  [32000/60000]\n",
            "Loss: 0.3706  [38400/60000]\n",
            "Loss: 0.3726  [44800/60000]\n",
            "Loss: 0.3601  [51200/60000]\n",
            "Loss: 0.3445  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.5%, Avg loss: 0.414324 \n",
            "\n",
            "Done !\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Loss: 0.3527  [6400/60000]\n",
            "Loss: 0.3415  [12800/60000]\n",
            "Loss: 0.3316  [19200/60000]\n",
            "Loss: 0.3314  [25600/60000]\n",
            "Loss: 0.3494  [32000/60000]\n",
            "Loss: 0.3323  [38400/60000]\n",
            "Loss: 0.3283  [44800/60000]\n",
            "Loss: 0.3385  [51200/60000]\n",
            "Loss: 0.3389  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.363029 \n",
            "\n",
            "Done !\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Loss: 0.3051  [6400/60000]\n",
            "Loss: 0.3139  [12800/60000]\n",
            "Loss: 0.3207  [19200/60000]\n",
            "Loss: 0.3190  [25600/60000]\n",
            "Loss: 0.2972  [32000/60000]\n",
            "Loss: 0.3258  [38400/60000]\n",
            "Loss: 0.3310  [44800/60000]\n",
            "Loss: 0.3088  [51200/60000]\n",
            "Loss: 0.2995  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.345677 \n",
            "\n",
            "Done !\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Loss: 0.2911  [6400/60000]\n",
            "Loss: 0.2940  [12800/60000]\n",
            "Loss: 0.2910  [19200/60000]\n",
            "Loss: 0.2944  [25600/60000]\n",
            "Loss: 0.2814  [32000/60000]\n",
            "Loss: 0.2903  [38400/60000]\n",
            "Loss: 0.3041  [44800/60000]\n",
            "Loss: 0.2862  [51200/60000]\n",
            "Loss: 0.2978  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.359765 \n",
            "\n",
            "Done !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3ru8-PfjkCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmspMu7zUiaexaTNx8sQsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}